<!-- build time:Thu Aug 10 2023 15:34:41 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="../images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="../images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="绯色鱼的博客" href="https://colsons.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="绯色鱼的博客" href="https://colsons.github.io/atom.xml"><link rel="alternate" type="application/json" title="绯色鱼的博客" href="https://colsons.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="../css/app.css?v=0.2.5"><meta name="keywords" content="python,torch,transformers,NLP"><link rel="canonical" href="https://colsons.github.io/archives/14a38ce5.html"><title>NLP-Transformers-多标签多分类模型搭建与训练 - 深度学习 | Lab Chen = 绯色鱼的博客</title><meta name="generator" content="Hexo 5.4.0"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">NLP-Transformers-多标签多分类模型搭建与训练</h1><div class="meta"><span class="item" title="创建时间: 2023-08-07 15:51:40"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2023-08-07T15:51:40+08:00">2023-08-07</time></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Lab Chen</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><img src="http://img.colsons.top/blog/1691649124lkEtBnfuQEMZfGhH"></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="../index.html">首页</a></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="item" rel="index" title="分类于 深度学习"><span itemprop="name">深度学习</span></a><meta itemprop="position" content="1"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="en"><link itemprop="mainEntityOfPage" href="https://colsons.github.io/archives/14a38ce5.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="../images/avatar.jpg"><meta itemprop="name" content="绯色鱼"><meta itemprop="description" content=", "></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="绯色鱼的博客"></span><div class="body md" itemprop="articleBody"><p>NLP 多标签多分类使用样例</p><span id="more"></span><p>因为是医学数据，涉及最大的问题，类别不均衡问题，提供三种解决方案：</p><ul><li><p>重采样：原数据集设置为 X，划分好训练集，测试集；以训练集为例如果需要 1000 个，训练集一共 5 类，每类中随机取一次，取 200 次。这样训练时的数据是均衡的，但是不适合多标签。</p></li><li><p>调整训练过程的权重分配，数量少的类别 权重大一些，同样不适合多标签。</p></li><li><p>Facol Loss：通过降低容易分类的样本的权重，提高难分类样本的权重，来解决类别不均衡问题。这有助于模型更好地关注少数类别样本。</p></li><li><p>Kappa Loss：这是基于 Cohen's Kappa 系数的一种损失函数。它可以用于处理类别不均衡问题，并鼓励模型在少数类别上的准确分类</p></li></ul><p>python 代码</p><pre class="language-language-python"><code class="language-language-python">import torch
import torch.nn as nn
import torch.optim as optim

class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        
    def forward(self, inputs, targets):
        ce_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return torch.mean(focal_loss)
        elif self.reduction == 'sum':
            return torch.sum(focal_loss)
        return focal_loss


class KappaLoss(nn.Module):
    def __init__(self):
        super(KappaLoss, self).__init__()

    def forward(self, y_pred, y_true, epsilon=1e-6):
        observed_agreement = torch.sum(y_pred * y_true, dim=0)
        expected_agreement = torch.sum(y_pred, dim=0) * torch.sum(y_true, dim=0)

        total_samples = y_pred.size(0)
        chance_agreement = torch.sum((expected_agreement / (total_samples ** 2)), dim=0)

        kappa = (observed_agreement - chance_agreement) / (expected_agreement - chance_agreement + epsilon)
        kappa_loss = 1 - torch.mean(kappa)
        
        return kappa_loss
</code></pre><p>其他参考代码</p><pre class="language-language-python"><code class="language-language-python">import os,sys,time
import re
import json, jsonlines
import numpy as np
import pandas as pd 
from pprint import pprint

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from torch.utils.tensorboard import SummaryWriter

from sklearn.preprocessing import OneHotEncoder
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, BertTokenizer
from keras.utils.np_utils import to_categorical


def pretreatment(comments):
    result_comments=[]
    punctuation='。，？！：%&~（）、；“”&n|\,.?!:%&~();""'
    for comment in comments:
        comment= ''.join([c for c in comment if c not in punctuation])
        comment= ''.join(comment.split())   #\xa0
        result_comments.append(comment)
    return result_comments


class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        
    def forward(self, inputs, targets):
        ce_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return torch.mean(focal_loss)
        elif self.reduction == 'sum':
            return torch.sum(focal_loss)
        return focal_loss
    
# 设置训练相关参数
epochs = 100
learning_rate = 1e-5
save_path = './results/'
save_epoch = 20   # 每xepoch保存一次模型参数

# 加载模型
model_name = "./premodel/bert-base-multilingual-cased"
tokenizer = BertTokenizer.from_pretrained(model_name)
num_labels = 12   # 多标签转one-hot后的维度
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)


# 加载数据
clean_data = pd.read_csv('你自己的数据')

datax = clean_data['txt']
datay = clean_data.values[:,1:]
datax = np.array(pretreatment(datax))
datay = to_categorical(datay)
datay = datay.reshape(datay.shape[0],datay.shape[1]*datay.shape[2])

pr = int(0.6*datax.shape[0])
x_train = datax[:pr].reshape(datax[:pr].shape[0],1)
y_train = datay[:pr].reshape(datax[:pr].shape[0],num_labels)
x_test = datax[pr:].reshape(datax[pr:].shape[0],1)
y_test = datay[pr:].reshape(datax[pr:].shape[0],num_labels)

# 数据集构建
x_train = pretreatment(x_train)
train_encoded_inputs = tokenizer.batch_encode_plus(x_train, padding=True, truncation=True, return_tensors="pt")
train_dataset = TensorDataset(train_encoded_inputs["input_ids"], train_encoded_inputs["attention_mask"], torch.tensor(y_train.tolist()))
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

x_test = pretreatment(x_test)
test_encoded_inputs = tokenizer.batch_encode_plus(x_test, padding=True, truncation=True, return_tensors="pt")
test_dataset = TensorDataset(test_encoded_inputs["input_ids"], test_encoded_inputs["attention_mask"], torch.tensor(y_test.tolist()))
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# 定义优化器和损失函数
optimizer = AdamW(model.parameters(), lr=learning_rate)
criterion = FocalLoss(alpha=1, gamma=2)
# criterion = KappaLoss()
# criterion = nn.CrossEntropyLoss() # 被替换掉的常用loss，交叉熵损失函数

# 开始训练
model_history = &#123;'epoch':[], 'train_loss':[], 'test_loss':[]&#125;
current_time_seconds = time.time()
current_time_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(current_time_seconds))
try:
    os.mkdir(save_path+current_time_str)
    print(save_path+current_time_str," 创建成功.")
except:
    pass

writer = SummaryWriter('logs')
all_time = 0
for epoch in range(epochs):
    model.train()
    train_loss = 0
    t1 = time.time()
    for batch in train_loader:
        optimizer.zero_grad()
        inputs = &#123;
            "input_ids": batch[0],
            "attention_mask": batch[1],
            "labels": batch[2]
        &#125;
        # labels = batch[2].to(device)
        labels = batch[2]
        outputs = model(**inputs)
        logits = outputs.logits
        
        train_step_loss = criterion(logits, labels)
        train_loss += train_step_loss.item()

        optimizer.zero_grad()
        train_step_loss.backward()
        optimizer.step()

    model.eval()
    test_loss = 0
    with torch.no_grad():
        for batch in test_loader:
            inputs = &#123;
                "input_ids": batch[0],
                "attention_mask": batch[1],
                "labels": batch[2]
            &#125;
            # labels = batch[2].to(device)
            labels = batch[2]
            optimizer.zero_grad()
            outputs = model(**inputs)
            logits = outputs.logits
            
            test_step_loss = criterion(logits, labels)
            test_loss += test_step_loss.item()
            optimizer.step()  
        
    train_avg_loss = train_loss / len(train_loader)
    test_avg_loss = test_loss / len(test_loader)

    model_history['epoch'].append(epoch+1)
    model_history['train_loss'].append(train_avg_loss)
    model_history['test_loss'].append(test_avg_loss)
    t2 = time.time()
    all_time = all_time + t2 - t1
    # 打印训练日志
    print(f"Epoch &#123;epoch+1&#125;/&#123;epochs&#125;, Train-Loss: &#123;train_avg_loss:.4f&#125;, Test-Loss: &#123;test_avg_loss:.4f&#125;, make time: &#123;t2-t1:.4f&#125;s, all time: &#123;all_time:.4f&#125;s")
    
    # 保存日志到tensorboard
    writer.add_scalar('TrainLoss', train_avg_loss, epoch+1)
    writer.add_scalar('TestLoss', test_avg_loss, epoch+1)
    
    # 每 save_epoch 保存一次模型 
    if (epoch+1)%save_epoch==0:
        try:
            os.mkdir(save_path+current_time_str+'/'+str(epoch+1))
        except:
            pass
        model.save_pretrained(save_path+current_time_str+'/'+str(epoch+1))
        tokenizer.save_pretrained(save_path+current_time_str+'/'+str(epoch+1))
        print("Epoch-"+str(epoch+1)+" 模型和分词器已保存.")
writer.close()            
# tensorboard --logdir=logs --host=0.0.0.0 --port=6006

with open(save_path+current_time_str+'/history.json', 'w') as json_file:
    json.dump(model_history, json_file)
print("训练过程已保存到 history.json 文件中.")

model.save_pretrained(save_path+current_time_str)
tokenizer.save_pretrained(save_path+current_time_str)
print("模型和分词器已保存.")
</code></pre><div class="tags"><a href="../tags/python/" rel="tag"><i class="ic i-tag"></i> python</a> <a href="../tags/torch/" rel="tag"><i class="ic i-tag"></i> torch</a> <a href="../tags/transformers/" rel="tag"><i class="ic i-tag"></i> transformers</a> <a href="../tags/NLP/" rel="tag"><i class="ic i-tag"></i> NLP</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间: 2023-08-10 14:32:27" itemprop="dateModified" datetime="2023-08-10T14:32:27+08:00">2023-08-10</time> </span><span id="archives/14a38ce5.html" class="item leancloud_visitors" data-flag-title="NLP-Transformers-多标签多分类模型搭建与训练" title="阅读次数"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">阅读次数</span> <span class="leancloud-visitors-count"></span> <span class="text">次</span></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]OoO</p><div id="qr"><div><img data-src="../images/wechatpay.png" alt="绯色鱼 微信支付"><p>微信支付</p></div><div><img data-src="../images/alipay.png" alt="绯色鱼 支付宝"><p>支付宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者: </strong>绯色鱼 <i class="ic i-at"><em>@</em></i>绯色鱼的博客</li><li class="link"><strong>本文链接: </strong><a href="../https:/colsons.github.io/archives/14a38ce5.html" title="NLP-Transformers-多标签多分类模型搭建与训练">https://colsons.github.io/archives/14a38ce5.html</a></li><li class="license"><strong>版权声明: </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="75f60fdc.html" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1giclh0m9pdj20zk0m8hdt.jpg" title="VUE2的Hello World"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> 前端</span><h3>VUE2的Hello World</h3></a></div><div class="item right"></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="68fa613c.html" rel="bookmark" title="「转载」Bert中文情感分类分析-tf1.14">「转载」Bert中文情感分类分析-tf1.14</a></li><li><a href="bc57630c.html" rel="bookmark" title="LSTM 昇腾平台全框架指南">LSTM 昇腾平台全框架指南</a></li><li class="active"><a href="" rel="bookmark" title="NLP-Transformers-多标签多分类模型搭建与训练">NLP-Transformers-多标签多分类模型搭建与训练</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="绯色鱼" data-src="../images/avatar.jpg"><p class="name" itemprop="name">绯色鱼</p><div class="description" itemprop="description"></div></div><nav class="state"><div class="item posts"><a href="../archives/"><span class="count">37</span> <span class="name">文章</span></a></div><div class="item categories"><a href="../categories/"><span class="count">10</span> <span class="name">分类</span></a></div><div class="item tags"><a href="../tags/"><span class="count">31</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL2NvbHNvbnM=" title="https:&#x2F;&#x2F;github.com&#x2F;colsons"><i class="ic i-github"></i></span> <span class="exturl item email" data-url="bWFpbHRvOjMzNTYwMTYzNjlAcXEuY29t" title="mailto:3356016369@qq.com"><i class="ic i-envelope"></i></span></div><ul class="menu"><li class="item"><a href="../index.html" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>汇总</a><ul class="submenu"><li class="item"><a href="../categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="../tags/" rel="section"><i class="ic i-tags"></i>标签</a></li><li class="item"><a href="../archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li></ul></li><li class="item"><a href="../friends/" rel="section"><i class="ic i-heart"></i>码友</a></li><li class="item"><a href="../links/" rel="section"><i class="ic i-magic"></i>链接</a></li><li class="item"><a href="../about/" rel="section"><i class="ic i-user"></i>关于</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2023</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">绯色鱼 @ Lab Chen</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"archives/14a38ce5.html",favicon:{show:"home",hide:"home"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,fancybox:!0,copyright:'复制成功！不带杂质 <br> ps : 手动标注来源是对我最大的认同 <i class="ic i-creative-commons"></i>BY-NC-SA.',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="../js/app.js?v=0.2.5"></script></body></html><!-- rebuild by hrmmi -->